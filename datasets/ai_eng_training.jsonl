{"id": "task_001", "title": "Implement AMP Training Loop with Gradient Accumulation", "domain": "PyTorch", "description": "Develop a robust PyTorch training loop for fine-tuning a transformer model (e.g., RoBERTa) on a classification task. The implementation must utilize torch.cuda.amp for Automatic Mixed Precision to reduce memory usage and implement gradient accumulation to simulate larger batch sizes on limited hardware.", "requirements": ["Use torch.cuda.amp.GradScaler", "Implement accumulation steps logic", "Include checkpoint saving based on validation loss"], "difficulty": "Intermediate"}
{"id": "task_002", "title": "Hybrid Search RAG Pipeline Construction", "domain": "RAG / Vector DB", "description": "Build a Retrieval-Augmented Generation system that connects to a vector database (like Weaviate or Qdrant). Instead of simple cosine similarity, implement a hybrid search strategy that combines dense vector retrieval (semantic search) with sparse vector retrieval (BM25 keyword search) and applies a re-ranking step before feeding context to the LLM.", "requirements": ["Generate dense embeddings (e.g., OpenAI text-embedding-3)", "Generate sparse vectors (BM25/Splade)", "Implement alpha-weighting for hybrid score fusion"], "difficulty": "Advanced"}
{"id": "task_003", "title": "LangChain ReAct Agent with SQL Tools", "domain": "LangChain / Agents", "description": "Create a LangChain agent using the ReAct (Reasoning and Acting) paradigm. The agent needs access to a read-only SQL database via a custom tool to answer natural language queries about sales data. It must infer the schema, construct the SQL query, execute it, and interpret the results to the user.", "requirements": ["Define custom LangChain Tools for SQL execution", "Use SystemMessage to define agent constraints", "Implement error handling for malformed SQL queries"], "difficulty": "Intermediate"}
{"id": "task_004", "title": "Dynamic Masking Collate Function", "domain": "PyTorch", "description": "Write a custom PyTorch Dataset and a specific `collate_fn` for a Masked Language Modeling (MLM) pre-training task. The collate function should take a batch of tokenized text and dynamically apply masking (replacing tokens with [MASK]) on the fly, ensuring a 15% masking probability per batch iteration.", "requirements": ["Subclass torch.utils.data.Dataset", "Implement custom collate_fn", "Handle special tokens (CLS, SEP) exclusion from masking"], "difficulty": "Intermediate"}
{"id": "task_005", "title": "Context-Aware Chat Memory Implementation", "domain": "LangChain", "description": "Implement a conversational chain that utilizes a persistent vector store as long-term memory. When a user sends a message, the system should fetch relevant past conversation fragments from the vector DB based on semantic similarity and inject them into the current prompt window alongside a standard short-term buffer window.", "requirements": ["Vectorize conversation history", "Implement retrieval logic for past interactions", "Merge retrieved context with ConversationBufferWindowMemory"], "difficulty": "Advanced"}
{"task_id": "AI-001", "domain": "PyTorch", "title": "Custom Mixed-Precision Training Loop", "description": "Implement a raw PyTorch training loop for a Vision Transformer (ViT). The loop must utilize 'torch.cuda.amp' for Automatic Mixed Precision (AMP) to optimize memory usage. It requires manual implementation of gradient scaling, gradient clipping to prevent exploding gradients, and a cosine annealing learning rate scheduler with a warmup phase.", "complexity": "Intermediate", "technologies": ["PyTorch", "CUDA", "AMP"]}
{"task_id": "AI-002", "domain": "Vector DB / RAG", "title": "Hybrid Search with Metadata Filtering", "description": "Design an ingestion pipeline that chunks technical documentation using semantic splitting. Store these chunks in a vector database (e.g., Qdrant or Weaviate). Implement a retrieval system that utilizes Hybrid Search (combining sparse BM25 keyword matching with dense vector similarity) and applies strict metadata pre-filtering based on document version and creation date before ranking.", "complexity": "Advanced", "technologies": ["Qdrant", "Sentence Transformers", "BM25"]}
{"task_id": "AI-003", "domain": "LangChain Agents", "title": "Multi-Tool ReAct Agent for Data Analysis", "description": "Construct a LangChain agent using the ReAct (Reasoning + Acting) architecture. The agent must be equipped with two custom tools: a 'SQLQueryTool' to fetch raw data from a local SQLite database and a 'PythonREPLTool' to execute pandas code for data visualization. The agent needs to reason through a user request, query the DB, plot a graph, and return a summary.", "complexity": "Advanced", "technologies": ["LangChain", "OpenAI API", "SQLite"]}
{"task_id": "AI-004", "domain": "RAG Optimization", "title": "Query Transformation and Reranking Pipeline", "description": "Build an advanced RAG pipeline that addresses query ambiguity. Step 1: Use an LLM to rewrite the user's initial query into three distinct sub-queries. Step 2: Retrieve documents for all sub-queries from a ChromaDB vector store. Step 3: Deduplicate results and use a Cross-Encoder model to rerank the documents by relevance before feeding them into the generation context.", "complexity": "Expert", "technologies": ["LangChain", "ChromaDB", "Cross-Encoders"]}
{"task_id": "AI-005", "domain": "PyTorch / Optimization", "title": "Distributed Data Parallel (DDP) Refactoring", "description": "Refactor a single-GPU PyTorch training script to support Distributed Data Parallel (DDP) training across multiple nodes. The task involves setting up the process group initialization ('dist.init_process_group'), wrapping the model with 'DistributedDataParallel', and modifying the DataLoader to use a 'DistributedSampler' to ensure non-overlapping data batches across GPUs.", "complexity": "Expert", "technologies": ["PyTorch", "DDP", "Distributed Computing"]}
{"task_id": "AI-001", "title": "Custom Contrastive Loss Training Loop", "category": "PyTorch", "difficulty": "Advanced", "description": "Implement a manual PyTorch training loop for a Siamese network architecture. The task involves writing a custom Contrastive Loss class, managing the optimizer `step()` and `zero_grad()` manually, and implementing gradient clipping. You must also integrate a learning rate scheduler that updates per epoch and log the triplet loss metrics to TensorBoard.", "tech_stack": ["PyTorch", "TorchVision", "TensorBoard", "Python"]}
{"task_id": "AI-002", "title": "Hybrid RAG Pipeline with Cross-Encoder Reranking", "category": "RAG", "difficulty": "Intermediate", "description": "Build a Retrieval Augmented Generation (RAG) system that utilizes a hybrid search strategy. Combine dense vector retrieval (using embeddings) with sparse keyword retrieval (BM25). Implement a post-retrieval step using a Cross-Encoder model to rerank the top-k documents based on relevance to the query before feeding the context into the LLM.", "tech_stack": ["LangChain", "ChromaDB", "BM25", "HuggingFace Transformers"]}
{"task_id": "AI-003", "title": "SQL-Interfacing ReAct Agent", "category": "Agent Logic", "difficulty": "Advanced", "description": "Develop a LangChain agent using the ReAct (Reasoning and Acting) pattern. The agent must be equipped with a custom tool that executes read-only SQL queries against a SQLite database. The agent needs to parse natural language questions into valid SQL, execute the tool, interpret the tabular return data, and formulate a final natural language response.", "tech_stack": ["LangChain", "LangGraph", "OpenAI API", "SQLAlchemy"]}
{"task_id": "AI-004", "title": "Distributed Mixed-Precision Transformer Training", "category": "PyTorch", "difficulty": "Expert", "description": "Refactor a standard PyTorch Transformer training script to support multi-GPU training using `DistributedDataParallel` (DDP). Additionally, integrate `torch.cuda.amp` (Automatic Mixed Precision) to optimize memory usage and training speed. The implementation must handle gradient scaling and process synchronization across ranks correctly.", "tech_stack": ["PyTorch", "CUDA", "NCCL", "DDP"]}
{"task_id": "AI-005", "title": "Metadata-Aware Vector Ingestion and Querying", "category": "Vector DB", "difficulty": "Intermediate", "description": "Create a document ingestion pipeline that chunks text and extracts structured metadata (e.g., author, publication date, document type). Store the embeddings and metadata in a vector database like Pinecone or Qdrant. Implement a query function that accepts a user prompt and dynamic filter criteria to perform 'self-querying' retrieval, applying metadata filters alongside vector similarity search.", "tech_stack": ["Pinecone", "LangChain", "OpenAI Embeddings", "Pydantic"]}
{"id": 1, "title": "Custom PyTorch Training Loop with AMP and Gradient Accumulation", "category": "Model Training", "difficulty": "Intermediate", "description": "Implement a robust training loop in PyTorch for a Transformer-based model. The loop must handle memory constraints by integrating Automatic Mixed Precision (AMP) using `torch.cuda.amp.GradScaler` and simulating larger batch sizes via gradient accumulation steps.", "requirements": ["PyTorch", "CUDA", "Hugging Face Transformers"]}
{"id": 2, "title": "Hybrid Search RAG Pipeline Implementation", "category": "Vector DB / Retrieval", "difficulty": "Advanced", "description": "Architect a Retrieval Augmented Generation (RAG) system that utilizes a vector database (e.g., Qdrant or Chroma) to perform hybrid search. The system should combine dense vector similarity with sparse keyword matching (BM25), followed by a Cross-Encoder reranking step to optimize context relevance before feeding the LLM.", "requirements": ["LangChain", "Qdrant/Pinecone", "Sentence Transformers", "Cross-Encoders"]}
{"id": 3, "title": "Self-Corrective RAG Agent with LangGraph", "category": "Agent Logic", "difficulty": "Advanced", "description": "Develop a stateful agent workflow using LangGraph that implements self-reflection. The agent must retrieve documents, grade them for relevance, and decide to either generate an answer or rewrite the search query and re-retrieve if the initial context is insufficient.", "requirements": ["LangChain", "LangGraph", "OpenAI API/Local LLM"]}
{"id": 4, "title": "Fine-Tuning Embedding Models with Contrastive Loss", "category": "Model Training", "difficulty": "Expert", "description": "Create a PyTorch script to fine-tune a pre-trained sentence transformer on a domain-specific dataset. Implement a custom dataset loader that generates triplets (anchor, positive, negative) and utilize Triplet Margin Loss or InfoNCE loss to improve vector separation in the latent space.", "requirements": ["PyTorch", "Sentence Transformers", "Pandas"]}
{"id": 5, "title": "SQL Database Querying Agent", "category": "Agent Logic", "difficulty": "Intermediate", "description": "Build a LangChain agent equipped with a dynamic SQL toolkit. The agent should be able to inspect database schema tables, generate syntactically correct SQL queries based on natural language questions, execute them, and interpret the results to provide a human-readable summary.", "requirements": ["LangChain", "SQLAlchemy", "SQLite/PostgreSQL"]}
{"id": 1, "title": "Implement Gradient Accumulation", "category": "PyTorch", "difficulty": "Intermediate", "description": "Write a custom PyTorch training loop that simulates a larger batch size by accumulating gradients over multiple mini-batches before executing `optimizer.step()` and `optimizer.zero_grad()`."}
{"id": 2, "title": "Build a Hybrid Search Retriever", "category": "Vector DB", "difficulty": "Advanced", "description": "Configure a vector database (like Weaviate or Qdrant) to perform hybrid search, combining sparse keyword matching (BM25) with dense vector semantic similarity, and integrate it into a retrieval pipeline."}
{"id": 3, "title": "Construct a ReAct Agent", "category": "LangChain", "difficulty": "Intermediate", "description": "Initialize a LangChain agent using the ReAct (Reasoning and Acting) framework that utilizes a custom defined search tool and a calculator tool to answer multi-step reasoning questions."}
{"id": 4, "title": "Distributed Data Parallel (DDP) Setup", "category": "PyTorch", "difficulty": "Advanced", "description": "Refactor a standard single-GPU training script to support multi-GPU training using `torch.nn.parallel.DistributedDataParallel`, including process group initialization and sampler configuration."}
{"id": 5, "title": "Implement Cross-Encoder Re-ranking", "category": "RAG", "difficulty": "Intermediate", "description": "Enhance a RAG pipeline by adding a post-retrieval step where a Cross-Encoder model scores and re-orders the documents retrieved by the vector database to improve context relevance."}
{"id": 6, "title": "Create a Self-Querying Retriever", "category": "LangChain", "difficulty": "Intermediate", "description": "Use LangChain's `SelfQueryRetriever` to enable an LLM to parse a natural language query into a structured vector search query with specific metadata filters (e.g., 'articles published after 2023')."}
{"id": 7, "title": "Custom Contrastive Loss Function", "category": "PyTorch", "difficulty": "Intermediate", "description": "Implement a custom Triplet Loss or Contrastive Loss class inheriting from `nn.Module` to train an embedding model, ensuring correct handling of anchor, positive, and negative pairs."}
{"id": 8, "title": "Asynchronous Vector Ingestion", "category": "Vector DB", "difficulty": "Advanced", "description": "Build a robust ingestion pipeline using Python's `asyncio` to read documents, generate embeddings via API, and upsert vectors into a database like Pinecone or Milvus concurrently for high throughput."}
{"id": 9, "title": "Structured Output Parser Logic", "category": "LangChain", "difficulty": "Beginner", "description": "Define a Pydantic object and use LangChain's `PydanticOutputParser` to force an LLM agent to return responses in a strict JSON schema, including error handling for malformed outputs."}
{"id": 10, "title": "Model Checkpointing and Early Stopping", "category": "PyTorch", "difficulty": "Beginner", "description": "Add logic to a training loop to save the model state dictionary (`state_dict`) only when validation loss improves, and terminate training if no improvement occurs after a set 'patience' window."}
{"id": 11, "title": "Contextual Compression Implementation", "category": "RAG", "difficulty": "Advanced", "description": "Implement a `ContextualCompressionRetriever` in LangChain that uses an LLM to summarize or extract only relevant snippets from retrieved documents before passing them to the final generation step."}
{"id": 12, "title": "Multi-Agent Supervisor Workflow", "category": "LangChain", "difficulty": "Advanced", "description": "Design a LangGraph or LangChain workflow where a 'Supervisor' agent routes tasks to specific worker agents (e.g., a Coder and a Reviewer) and manages the state between them."}
{"id": 13, "title": "Custom Multi-Modal Dataset", "category": "PyTorch", "difficulty": "Intermediate", "description": "Subclass `torch.utils.data.Dataset` to handle a dataset containing paired images and text, implementing `__len__` and `__getitem__` to return image tensors and tokenized text IDs simultaneously."}
{"id": 14, "title": "SQL Database Agent Tool", "category": "LangChain", "difficulty": "Intermediate", "description": "Create a custom LangChain Tool that allows an agent to inspect database schemas and execute safe SQL queries to answer questions based on tabular data."}
{"id": 15, "title": "Recursive Character Chunking Strategy", "category": "RAG", "difficulty": "Beginner", "description": "Implement a text processing pipeline using `RecursiveCharacterTextSplitter` to optimize document chunk sizes and overlap windows for a specific LLM context window limit."}
{"id": 1, "category": "PyTorch", "title": "Implement Gradient Accumulation", "description": "Modify a standard PyTorch training loop to simulate a larger batch size by accumulating gradients over multiple mini-batches before performing an optimizer step.", "difficulty": "Intermediate"}
{"id": 2, "category": "RAG", "title": "Parent Document Retriever Strategy", "description": "Configure a retrieval system that indexes small chunks of text for semantic search but returns the larger 'parent' document or window of context to the LLM during generation.", "difficulty": "Advanced"}
{"id": 3, "category": "LangChain", "title": "Build a Custom SQL Tool", "description": "Create a custom LangChain Tool that accepts natural language, converts it to a SQL query, executes it against a SQLite database, and returns the result to the agent.", "difficulty": "Intermediate"}
{"id": 4, "category": "PyTorch", "title": "Mixed Precision Training Setup", "description": "Integrate `torch.cuda.amp` (Automatic Mixed Precision) into a training loop to scale gradients and autocast operations, reducing memory usage and speeding up training on GPUs.", "difficulty": "Intermediate"}
{"id": 5, "category": "Vector DB", "title": "Implement Hybrid Search", "description": "Set up a vector database (e.g., Weaviate or Pinecone) to perform a hybrid search that combines dense vector similarity with sparse keyword matching (BM25) with weighted scoring.", "difficulty": "Advanced"}
{"id": 6, "category": "LangChain", "title": "ReAct Agent Implementation", "description": "Construct an agent using the ReAct (Reasoning and Acting) framework that can iteratively reason about a user query, select external tools, and parse outputs to arrive at a final answer.", "difficulty": "Advanced"}
{"id": 7, "category": "PyTorch", "title": "Custom Contrastive Loss Function", "description": "Write a custom `nn.Module` class to implement SimCLR or Triplet Loss for training embeddings, handling positive and negative pairs within the forward pass.", "difficulty": "Expert"}
{"id": 8, "category": "RAG", "title": "Cross-Encoder Re-ranking Pipeline", "description": "Implement a two-stage retrieval process where the top-k results from a bi-encoder vector search are re-ranked using a computationally heavier cross-encoder model for higher accuracy.", "difficulty": "Intermediate"}
{"id": 9, "category": "LangChain", "title": "Conversation Summary Memory", "description": "Implement a memory module for a chatbot that progressively summarizes the conversation history using an LLM once the token count exceeds a defined threshold, rather than keeping the raw buffer.", "difficulty": "Intermediate"}
{"id": 10, "category": "PyTorch", "title": "Distributed Data Parallel (DDP) Refactoring", "description": "Refactor a single-GPU training script to support multi-GPU training across nodes using `torch.nn.parallel.DistributedDataParallel`, handling rank synchronization and sampler distribution.", "difficulty": "Expert"}
{"id": 11, "category": "RAG", "title": "Maximum Marginal Relevance (MMR) Retrieval", "description": "Implement an MMR algorithm during the retrieval step to select documents that are not only similar to the query but also diverse from each other to reduce redundancy in the context window.", "difficulty": "Intermediate"}
{"id": 12, "category": "LangChain", "title": "Router Chain Logic", "description": "Develop a routing chain that classifies an incoming user query (e.g., 'Physics' vs. 'History') and dynamically routes the prompt to specific sub-chains or vector stores optimized for that domain.", "difficulty": "Intermediate"}
{"id": 13, "category": "PyTorch", "title": "Early Stopping and Checkpointing", "description": "Add logic to the validation loop to monitor validation loss, save the model state dict only when the metric improves, and stop training if no improvement is seen after `patience` epochs.", "difficulty": "Beginner"}
{"id": 14, "category": "Vector DB", "title": "Multi-Query Expansion", "description": "Create a retrieval chain that uses an LLM to generate three different variations of a user's prompt, retrieves documents for all variations, and deduplicates the results before generation.", "difficulty": "Advanced"}
{"id": 15, "category": "LangChain", "title": "Human-in-the-Loop Tool Approval", "description": "Configure an agent executor that pauses execution and requires human approval (via input or API callback) before executing sensitive tools, such as writing to a database or sending an email.", "difficulty": "Advanced"}
{"id": 1, "category": "PyTorch Training", "title": "Gradient Accumulation Implementation", "description": "Modify a standard PyTorch training loop to implement gradient accumulation. This involves scaling the loss, calling backward() multiple times, and only stepping the optimizer after a defined number of accumulation steps to simulate a larger batch size.", "technologies": ["PyTorch", "Python"], "difficulty": "Intermediate"}
{"id": 2, "category": "Vector DB", "title": "Hybrid Search Retriever", "description": "Implement a custom retriever class that combines dense vector search (embeddings) with sparse keyword search (BM25). Normalize the scores from both methods and fuse them using Reciprocal Rank Fusion (RRF) before returning the top-k documents.", "technologies": ["Pinecone", "LangChain", "BM25"], "difficulty": "Advanced"}
{"id": 3, "category": "Agent Logic", "title": "Custom ReAct Agent Loop", "description": "Build a ReAct (Reasoning + Acting) loop from scratch without using pre-built agent executors. The loop should prompt the LLM to generate a 'Thought', 'Action', and 'Action Input', execute a defined Python function based on the action, and feed the 'Observation' back into the prompt context.", "technologies": ["LangChain", "OpenAI API"], "difficulty": "Advanced"}
{"id": 4, "category": "PyTorch Training", "title": "Mixed Precision Training Integration", "description": "Integrate `torch.cuda.amp.GradScaler` and `autocast` into an existing training script to enable FP16 mixed-precision training. Ensure that gradients are unscaled before clipping and that the scaler is updated correctly after optimizer steps.", "technologies": ["PyTorch", "CUDA"], "difficulty": "Intermediate"}
{"id": 5, "category": "RAG", "title": "Contextual Compression with MMR", "description": "Implement a retrieval post-processing step using Maximum Marginal Relevance (MMR). The goal is to rerank retrieved documents to maximize diversity while maintaining relevance to the query, reducing redundancy in the context window.", "technologies": ["LangChain", "FAISS", "NumPy"], "difficulty": "Intermediate"}
{"id": 6, "category": "Agent Logic", "title": "Pydantic Output Parser for Structured Actions", "description": "Create a LangChain agent that is strictly typed. Define a Pydantic object representing the desired output schema (e.g., a JSON with specific keys) and implement an output parser that validates the LLM's response, triggering a retry prompt if validation fails.", "technologies": ["LangChain", "Pydantic"], "difficulty": "Intermediate"}
{"id": 7, "category": "Vector DB", "title": "Recursive Character Chunking Pipeline", "description": "Write a data ingestion script that loads a PDF, cleans the text, and applies a recursive character text splitter. The splitter must respect sentence boundaries and overlap settings before batching the chunks for embedding and insertion into a vector database.", "technologies": ["LangChain", "ChromaDB", "Unstructured"], "difficulty": "Beginner"}
{"id": 8, "category": "PyTorch Training", "title": "Custom Learning Rate Scheduler with Warmup", "description": "Implement a custom learning rate scheduler that linearly warms up the learning rate from 0 to a target value over the first N epochs, and then applies cosine annealing decay for the remainder of the training steps.", "technologies": ["PyTorch"], "difficulty": "Intermediate"}
{"id": 9, "category": "Agent Logic", "title": "SQL Database Query Tool", "description": "Develop a custom LangChain Tool that connects to a SQLite database. The tool should accept a natural language query, convert it to a sanitized SQL query using an LLM chain, execute it, and return the rows as a string observation.", "technologies": ["LangChain", "SQLAlchemy", "SQLite"], "difficulty": "Advanced"}
{"id": 10, "category": "RAG", "title": "Hypothetical Document Embeddings (HyDE)", "description": "Build a retrieval chain that generates a hypothetical answer to the user's question using an LLM *before* embedding it. Use this hypothetical vector to query the vector store, improving retrieval performance for questions that look semantically different from the answers.", "technologies": ["LangChain", "Weaviate", "OpenAI"], "difficulty": "Advanced"}
{"id": 11, "category": "PyTorch Training", "title": "Early Stopping with Model Checkpointing", "description": "Add logic to the validation loop that tracks the validation loss. If the loss does not improve for a specified 'patience' number of epochs, terminate training and save the state dictionary of the best-performing model.", "technologies": ["PyTorch"], "difficulty": "Beginner"}
{"id": 12, "category": "Vector DB", "title": "Metadata Filtering Strategy", "description": "Design a vector store query that utilizes metadata filtering. The task involves tagging document chunks with metadata (e.g., year, author) during ingestion and constructing a retriever that filters results based on user-defined criteria before performing the vector similarity search.", "technologies": ["Qdrant", "LangChain"], "difficulty": "Intermediate"}
{"id": 13, "category": "Agent Logic", "title": "Conversation Summary Buffer Memory", "description": "Implement a memory module for a chatbot agent that maintains a buffer of recent messages. When the token count exceeds a threshold, trigger a background chain to summarize the oldest messages and append the summary to the system prompt, keeping the context window manageable.", "technologies": ["LangChain", "Redis"], "difficulty": "Intermediate"}
{"id": 14, "category": "PyTorch Training", "title": "Distributed Data Parallel (DDP) Setup", "description": "Refactor a single-GPU training script to support multi-GPU training using `torch.nn.parallel.DistributedDataParallel`. Handle process initialization, rank assignment, and ensure the DataLoader uses a `DistributedSampler`.", "technologies": ["PyTorch", "Distributed Computing"], "difficulty": "Expert"}
{"id": 15, "category": "RAG", "title": "Multi-Query Retriever", "description": "Create a RAG pipeline that uses an LLM to generate three different variations of a user's prompt. Retrieve documents for all three variations, deduplicate the results based on document ID, and feed the unique set of context into the final generation step.", "technologies": ["LangChain", "LCEL"], "difficulty": "Intermediate"}
{"id": 1, "title": "Implement Gradient Accumulation", "category": "PyTorch", "description": "Write a custom PyTorch training loop that simulates a larger batch size by accumulating gradients over multiple mini-batches before performing a `optimizer.step()` and `optimizer.zero_grad()`, enabling training of large models on limited VRAM.", "difficulty": "Intermediate", "technologies": ["PyTorch"]}
{"id": 2, "title": "Hybrid Search Retriever", "category": "RAG", "description": "Create a LangChain retriever that combines sparse keyword search (BM25) with dense vector similarity search (using Pinecone or Milvus). Implement a logic to fuse the scores (Reciprocal Rank Fusion) to reorder the final retrieved documents.", "difficulty": "Advanced", "technologies": ["LangChain", "Pinecone", "BM25"]}
{"id": 3, "title": "LangGraph Self-Correction Agent", "category": "Agent Logic", "description": "Build a state graph using LangGraph where an agent generates code, executes it, and if an error occurs, passes the error trace back to the generation node to iteratively fix the code until it runs successfully.", "difficulty": "Advanced", "technologies": ["LangChain", "LangGraph", "Python REPL"]}
{"id": 4, "title": "Mixed Precision Training (AMP)", "category": "PyTorch", "description": "Modify a standard PyTorch training loop to utilize `torch.cuda.amp.GradScaler` and `autocast`. The goal is to speed up training and reduce memory usage by dynamically switching between float16 and float32 during backpropagation.", "difficulty": "Intermediate", "technologies": ["PyTorch", "CUDA"]}
{"id": 5, "title": "Parent Document Retriever Strategy", "category": "RAG", "description": "Implement a RAG pipeline where small chunks are embedded for vector similarity search, but the retrieval step returns the larger 'parent' document associated with that chunk to provide better context to the LLM.", "difficulty": "Intermediate", "technologies": ["LangChain", "MongoDB", "ChromaDB"]}
{"id": 6, "title": "Custom Dynamic Masking DataLoader", "category": "PyTorch", "description": "Create a custom PyTorch `Dataset` and `collate_fn` for a Masked Language Modeling (MLM) task that dynamically masks tokens in the input text batch on-the-fly during training, rather than using pre-masked data.", "difficulty": "Intermediate", "technologies": ["PyTorch", "Transformers"]}
{"id": 7, "title": "SQL Database Agent with Sanitization", "category": "Agent Logic", "description": "Develop a LangChain agent equipped with a custom tool to query a SQL database. The tool must include a validation layer to sanitize inputs and a schema-inspection step to allow the LLM to understand the table structure before writing queries.", "difficulty": "Intermediate", "technologies": ["LangChain", "SQLAlchemy", "OpenAI"]}
{"id": 8, "title": "Qdrant Ingestion Pipeline with Metadata", "category": "Vector DB", "description": "Write a script to process a directory of PDF files. Use unstructured.io to extract text, chunk the text recursively, generate embeddings using OpenAI models, and upsert vectors into Qdrant with metadata payloads (page number, filename).", "difficulty": "Beginner", "technologies": ["Qdrant", "LangChain", "Unstructured"]}
{"id": 9, "title": "Distributed Data Parallel (DDP) Setup", "category": "PyTorch", "description": "Convert a single-GPU training script into a Distributed Data Parallel (DDP) compliant script. Implement `mp.spawn`, setup process groups, and wrap the model with `DistributedDataParallel` to train across multiple GPUs.", "difficulty": "Advanced", "technologies": ["PyTorch", "DDP"]}
{"id": 10, "title": "Cross-Encoder Reranking Pipeline", "category": "RAG", "description": "Enhance a standard RAG pipeline by adding a post-retrieval step. Retrieve the top 20 documents using vector similarity, then use a Cross-Encoder model (e.g., via Hugging Face) to score and re-rank them, passing only the top 5 to the LLM context.", "difficulty": "Intermediate", "technologies": ["LangChain", "Hugging Face", "Weaviate"]}
{"id": 11, "title": "Custom Tool with Pydantic Arguments", "category": "Agent Logic", "description": "Define a custom LangChain tool using the `@tool` decorator. The tool should accept complex input parameters defined by a Pydantic model (e.g., a travel booking tool requiring date, destination, and budget) and return structured JSON output.", "difficulty": "Beginner", "technologies": ["LangChain", "Pydantic"]}
{"id": 12, "title": "Early Stopping and Checkpointing", "category": "PyTorch", "description": "Implement a `Trainer` class wrapper around a PyTorch loop that tracks validation loss. It should save the model state dict whenever validation loss improves and terminate training if loss hasn't improved for `patience` epochs.", "difficulty": "Beginner", "technologies": ["PyTorch"]}
{"id": 13, "title": "Plan-and-Solve Agent Architecture", "category": "Agent Logic", "description": "Implement a 'Plan-and-Solve' agent strategy where the LLM first generates a step-by-step plan to answer a complex user query, and then sequentially executes tools for each step of the plan, maintaining state between steps.", "difficulty": "Advanced", "technologies": ["LangChain", "GPT-4"]}
{"id": 14, "title": "Vector Store Metadata Filtering", "category": "Vector DB", "description": "Implement a query interface for a ChromaDB store that accepts natural language queries and dynamic filter criteria. Programmatically construct the metadata filter dictionary (e.g., `{'author': 'John', 'year': {'$gt': 2020}}`) to apply before the vector search.", "difficulty": "Intermediate", "technologies": ["ChromaDB", "LangChain"]}
{"id": 15, "title": "RunnableParallel Async Chains", "category": "LangChain", "description": "Use LangChain Expression Language (LCEL) to create a `RunnableParallel` chain. The chain should fetch context from a vector store and a web search API concurrently (asynchronously) before passing both contexts to a final prompt template.", "difficulty": "Intermediate", "technologies": ["LangChain", "LCEL", "AsyncIO"]}
{"id": 1, "category": "PyTorch", "title": "Implement Gradient Accumulation", "description": "Modify a standard PyTorch training loop to support gradient accumulation, allowing for larger effective batch sizes on hardware with limited VRAM by updating weights only after N forward/backward passes.", "difficulty": "Intermediate"}
{"id": 2, "category": "RAG", "title": "Hybrid Search Implementation", "description": "Configure a vector database (e.g., Pinecone or Weaviate) to perform hybrid search, combining dense vector similarity with sparse keyword matching (BM25) to improve retrieval accuracy for domain-specific terms.", "difficulty": "Intermediate"}
{"id": 3, "category": "LangChain", "title": "Custom Tool Creation for Agents", "description": "Develop a custom LangChain tool using the @tool decorator that allows an LLM agent to query an internal SQL database safely, including input schema validation using Pydantic.", "difficulty": "Intermediate"}
{"id": 4, "category": "PyTorch", "title": "QLoRA Fine-Tuning Setup", "description": "Set up a PEFT (Parameter-Efficient Fine-Tuning) workflow to fine-tune a 7B parameter LLM using 4-bit quantization (QLoRA) and LoRA adapters on a custom instruction dataset.", "difficulty": "Advanced"}
{"id": 5, "category": "RAG", "title": "Parent Document Retriever", "description": "Implement a Parent Document Retriever architecture where small text chunks are embedded for search, but the larger parent document (or window) is retrieved and passed to the LLM for context coherence.", "difficulty": "Advanced"}
{"id": 6, "category": "LangChain", "title": "Conversation Summary Buffer Memory", "description": "Integrate memory into a conversational chain that maintains a buffer of recent messages while summarizing older interactions to keep the context window usage constant over long conversations.", "difficulty": "Beginner"}
{"id": 7, "category": "PyTorch", "title": "Custom Contrastive Loss Function", "description": "Write a custom PyTorch module for Triplet Margin Loss or InfoNCE loss to train an embedding model, ensuring positive and negative pairs are handled correctly within the batch.", "difficulty": "Advanced"}
{"id": 8, "category": "Agents", "title": "Multi-Agent Code Reviewer", "description": "Orchestrate a multi-agent system (using LangGraph or LangChain) where one agent generates Python code based on a prompt, and a second 'critic' agent reviews the code for bugs and security vulnerabilities.", "difficulty": "Advanced"}
{"id": 9, "category": "RAG", "title": "Cross-Encoder Reranking", "description": "Enhance a RAG pipeline by adding a post-retrieval step that uses a Cross-Encoder model (e.g., BGE-Reranker) to re-score and re-order the top-k retrieved documents before sending them to the generator.", "difficulty": "Intermediate"}
{"id": 10, "category": "PyTorch", "title": "Distributed Data Parallel (DDP) Setup", "description": "Refactor a single-GPU training script to support DistributedDataParallel (DDP) to train a model across multiple GPUs on a single node, handling rank synchronization and data sampling.", "difficulty": "Advanced"}
{"id": 11, "category": "LangChain", "title": "Structured Output Parsing", "description": "Implement a PydanticOutputParser to force an LLM to generate responses in a strict JSON format matching a specific data class, handling parsing errors with auto-retry logic.", "difficulty": "Beginner"}
{"id": 12, "category": "RAG", "title": "Contextual Compression", "description": "Apply a Contextual Compression Retriever to filter out irrelevant information from retrieved documents, passing only the specific sentences or paragraphs relevant to the query to the LLM.", "difficulty": "Intermediate"}
{"id": 13, "category": "PyTorch", "title": "Early Stopping & Checkpointing", "description": "Implement a callback mechanism in a raw PyTorch training loop that monitors validation loss, saves the best model state dict, and halts training if loss does not improve for 'patience' epochs.", "difficulty": "Beginner"}
{"id": 14, "category": "Agents", "title": "Plan-and-Execute Agent", "description": "Build an agent that breaks down a complex user query into a multi-step plan, executes each step sequentially using appropriate tools, and synthesizes the final answer.", "difficulty": "Advanced"}
{"id": 15, "category": "RAG", "title": "Metadata Filtering Strategy", "description": "Design an ingestion pipeline that extracts metadata (year, author, source) from documents and implements a retriever that applies pre-filtering on vector search queries based on user constraints.", "difficulty": "Beginner"}
{"id": 1, "category": "PyTorch", "title": "Implement Gradient Accumulation", "description": "Modify a standard PyTorch training loop to simulate a larger batch size by accumulating gradients over multiple mini-batches before executing `optimizer.step()` and `optimizer.zero_grad()`.", "difficulty": "Intermediate"}
{"id": 2, "category": "RAG", "title": "Hybrid Search Implementation", "description": "Configure a vector database (e.g., Weaviate or Pinecone) to perform hybrid search by combining sparse vectors (BM25/Keyword) with dense vectors (Semantic Embedding) and implement the alpha-weighting logic.", "difficulty": "Advanced"}
{"id": 3, "category": "LangChain", "title": "Custom Agent Tool Creation", "description": "Define a custom tool using the `@tool` decorator or `BaseTool` class that connects to an external weather API, including proper argument schema definitions for the LLM to understand input requirements.", "difficulty": "Intermediate"}
{"id": 4, "category": "PyTorch", "title": "Mixed Precision Training Integration", "description": "Integrate `torch.cuda.amp.GradScaler` and `autocast` into a training loop to enable FP16 mixed precision training, ensuring stability by handling gradient scaling.", "difficulty": "Intermediate"}
{"id": 5, "category": "RAG", "title": "Contextual Compression Pipeline", "description": "Build a LangChain retrieval pipeline that uses a Document Compressor (e.g., a cross-encoder reranker) to filter and rank retrieved documents before passing them to the context window.", "difficulty": "Advanced"}
{"id": 6, "category": "LangChain", "title": "ReAct Agent with Memory", "description": "Construct a ReAct (Reasoning and Acting) agent that utilizes a `ConversationBufferWindowMemory` to maintain context over a multi-turn conversation while using tools.", "difficulty": "Intermediate"}
{"id": 7, "category": "PyTorch", "title": "Custom Dataset with Dynamic Augmentation", "description": "Create a custom `torch.utils.data.Dataset` class for image data that applies random augmentations (rotation, flip, color jitter) on-the-fly in the `__getitem__` method.", "difficulty": "Beginner"}
{"id": 8, "category": "Vector DB", "title": "Parent Document Retriever Strategy", "description": "Implement a 'Parent Document' indexing strategy where small chunks are embedded for search, but the full parent document is retrieved and passed to the LLM for generation.", "difficulty": "Advanced"}
{"id": 9, "category": "LangChain", "title": "Structured Output Parser", "description": "Create a chain utilizing PydanticOutputParser to force an LLM to generate a response adhering to a specific JSON schema, including retry logic for parsing errors.", "difficulty": "Intermediate"}
{"id": 10, "category": "PyTorch", "title": "Distributed Data Parallel (DDP) Setup", "description": "Refactor a single-GPU training script to support multi-GPU training using `torch.nn.parallel.DistributedDataParallel`, handling process group initialization and sampler distribution.", "difficulty": "Expert"}
{"id": 11, "category": "RAG", "title": "Multi-Query Retriever", "description": "Implement a module that uses an LLM to generate three different variations of a user's query, retrieves documents for all variations, and deduplicates the results.", "difficulty": "Intermediate"}
{"id": 12, "category": "LangChain", "title": "Router Chain Logic", "description": "Design a Router Chain that classifies an input query (e.g., 'Math', 'Physics', 'History') and dynamically routes the request to a specialized sub-chain with a distinct prompt template.", "difficulty": "Intermediate"}
{"id": 13, "category": "PyTorch", "title": "Custom Contrastive Loss Function", "description": "Implement a Triplet Margin Loss or Contrastive Loss function from scratch in PyTorch for training Siamese networks on paired data.", "difficulty": "Advanced"}
{"id": 14, "category": "Vector DB", "title": "Metadata Filtering Logic", "description": "Write a query function that performs vector similarity search constrained by complex metadata filters (e.g., specific date ranges or author tags) using ChromaDB or Qdrant.", "difficulty": "Beginner"}
{"id": 15, "category": "LangChain", "title": "Human-in-the-Loop Agent", "description": "Implement a LangGraph workflow or agent loop that pauses execution before a critical tool action (e.g., sending an email) to require human approval via input.", "difficulty": "Expert"}
