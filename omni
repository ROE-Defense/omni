#!/usr/bin/env python3
import os
import sys
import time
import glob

# Omni - The Agentic Interface
# Usage: ./omni.py

BANNER = r"""
   ____  __  __ _   _ _____ 
  / __ \|  \/  | \ | |_   _|
 | |  | | \  / |  \| | | |  
 | |  | | |\/| | . ` | | |  
 | |__| | |  | | |\  |_| |_ 
  \____/|_|  |_|_| \_|_____|
"""

class OmniAgent:
    def __init__(self):
        self.active_brain = None
        self.history = []
        self.context_files = []
        self.available_brains = self.scan_brains()

    def scan_brains(self):
        """Auto-discover fused models."""
        brains = []
        if os.path.exists("models"):
            for p in glob.glob("models/*-fused"):
                name = os.path.basename(p).replace("-fused", "")
                brains.append(name)
        return brains

    def scan_context(self):
        """Quickly scan CWD for relevant files."""
        # Simple heuristic for demo
        files = [f for f in os.listdir(".") if os.path.isfile(f) and not f.startswith(".")]
        self.context_files = files
        return len(files)

    def route_intent(self, user_input):
        """
        The 'Router' Logic.
        Decides which brain or action to take based on natural language.
        In a real version, a small 1B model (Router) would do this classification.
        For now, we use keyword heuristics.
        """
        u = user_input.lower()
        
        # 1. System Commands
        if u in ["menu", "help", "options", "ls", "list"]:
            return "MENU"
        if u in ["exit", "quit", "q"]:
            return "EXIT"
        if u in ["status", "stat"]:
            return "STATUS"

        # 2. Brain Switching
        # Check if user explicitly asked for a brain
        for b in self.available_brains:
            if b in u:
                return f"LOAD:{b}"
        
        # 3. Domain Heuristics (Auto-Routing)
        if any(x in u for x in ["swift", "ios", "iphone", "apple"]):
            return "LOAD:ios"
        if any(x in u for x in ["android", "kotlin", "apk"]):
            return "LOAD:android"
        if any(x in u for x in ["flutter", "dart", "widget"]):
            return "LOAD:flutter"
        if any(x in u for x in ["react", "frontend", "css", "html", "ui"]):
            return "LOAD:frontend"
        if any(x in u for x in ["python", "api", "backend", "sql", "db"]):
            return "LOAD:backend"
        if any(x in u for x in ["docker", "deploy", "k8s", "pipeline"]):
            return "LOAD:devops"
        if any(x in u for x in ["design", "architecture", "cloud", "aws"]):
            return "LOAD:architect"
        if any(x in u for x in ["shell", "bash", "terminal", "command"]):
            return "LOAD:shell"

        # 4. Default to Chat
        return "CHAT"

    def load_brain(self, name):
        if name not in self.available_brains:
            print(f"âš ï¸  Brain '{name}' not found locally.")
            return False
        
        print(f"âš¡ Loading Cognitive Cartridge: @roe/{name}...")
        # Simulation of loading time
        time.sleep(0.5) 
        self.active_brain = name
        print(f"âœ… Active: @roe/{name}")
        return True

    def generate_response(self, prompt):
        """
        The Inference Engine.
        Here we would call mlx_lm.generate.
        """
        if not self.active_brain:
            print("ðŸ¤– (Router) I need to pick a brain first...")
            # If no brain selected, try to route or default to Architect
            action = self.route_intent(prompt)
            if action.startswith("LOAD:"):
                target = action.split(":")[1]
                if self.load_brain(target):
                    pass # Continue to gen
                else:
                    print("âŒ Could not auto-load brain. Please select one manually.")
                    return
            else:
                # Fallback
                self.load_brain("architect")

        print(f"ðŸ§  (@roe/{self.active_brain}) Thinking...")
        # MOCK RESPONSE
        time.sleep(1)
        print(f"\n> Here is a suggested solution based on your {self.active_brain} context:")
        print(f"> [Simulated code/text response for '{prompt}']\n")

    def show_menu(self):
        print("\n--- OMNI MENU ---")
        print(f"Active Brain: {self.active_brain or 'None (Auto)'}")
        print("Available Cartridges:")
        for b in self.available_brains:
            print(f" â€¢ {b}")
        print("\nCommands:")
        print(" â€¢ 'menu' -> Show this list")
        print(" â€¢ 'status' -> System health")
        print(" â€¢ 'exit' -> Quit")
        print("-----------------\n")

    def train_brain(self, name, path):
        """
        The 'Train' Logic.
        Converts user documents into a LoRA adapter.
        """
        import subprocess
        
        print(f"\nðŸ§  INITIATING TRAINING SEQUENCE for @my/{name}")
        print(f"ðŸ“‚ Source: {path}")
        
        # 1. Validation
        if not os.path.exists(path):
            print(f"âŒ Error: Path '{path}' does not exist.")
            return

        # 2. Ingestion (using cpack logic)
        print("READING DOCUMENTS...")
        # Simulate packing for now (in real version, import cpack)
        file_count = 0
        for root, dirs, files in os.walk(path):
            file_count += len(files)
        print(f"âœ… Found {file_count} files.")

        # 3. Dataset Generation
        print("GENERATING DATASET...")
        dataset_path = f"datasets/{name}_custom.jsonl"
        
        # Create a dummy dataset for MVP
        with open(dataset_path, "w") as f:
            # In a real version, we'd use an LLM to generate Q&A from the files
            import json
            sample = {"messages": [{"role": "user", "content": f"Explain the code in {path}"}, {"role": "assistant", "content": "This is a custom trained response based on your data."}]}
            for _ in range(100): # Minimum viable dataset
                f.write(json.dumps(sample) + "\n")
        
        print(f"âœ… Dataset created: {dataset_path} (100 samples)")

        # 4. Training
        print("STARTING LoRA FINE-TUNING...")
        print("(This may take 10-20 minutes on Apple Silicon)")
        
        # Check for train_env
        train_cmd = "train_env/bin/mlx_lm.lora"
        if not os.path.exists(train_cmd):
             print("âŒ Error: Training environment not found. Run 'omni setup' first.")
             return

        cmd = [
            train_cmd,
            "--model", "mlx-community/Llama-3.2-3B-Instruct", # Default to 3B (Proven working)
            "--train",
            "--data", f"datasets/{name}_custom", 
            "--iters", "100",
            "--batch-size", "1",
            "--adapter-path", f"adapters/{name}"
        ]
        
        # Create data dir structure for MLX
        os.makedirs(f"data/{name}", exist_ok=True)
        os.system(f"cp {dataset_path} data/{name}/train.jsonl")
        os.system(f"cp {dataset_path} data/{name}/valid.jsonl")
        
        try:
            # We use Popen to stream output
            process = subprocess.Popen(
                [train_cmd, "--model", "mlx-community/Llama-3.2-3B-Instruct", "--train", "--data", f"data/{name}", "--iters", "100", "--adapter-path", f"adapters/{name}"],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Show a progress spinner or just wait
            print("Training...", end="", flush=True)
            while process.poll() is None:
                time.sleep(1)
                print(".", end="", flush=True)
            print("\n")
            
            if process.returncode == 0:
                print(f"âœ… TRAINING COMPLETE!")
                print(f"ðŸš€ New Brain Available: @my/{name}")
                print(f"To use it: omni run -> 'load {name}'")
                
                # Register it
                self.available_brains.append(name)
            else:
                print(f"âŒ Training Failed. Check logs.")
                print(process.stderr.read())

        except Exception as e:
            print(f"âŒ Execution Error: {e}")

    def run(self):
        # CLI Argument Handling for non-interactive modes
        if len(sys.argv) > 1:
            cmd = sys.argv[1]
            if cmd == "train":
                # omni train --name foo --path bar
                if "--name" in sys.argv and "--path" in sys.argv:
                    try:
                        n_idx = sys.argv.index("--name") + 1
                        p_idx = sys.argv.index("--path") + 1
                        name = sys.argv[n_idx]
                        path = sys.argv[p_idx]
                        self.train_brain(name, path)
                        return
                    except IndexError:
                        print("Usage: omni train --name <name> --path <path>")
                        return
                else:
                    print("Usage: omni train --name <name> --path <path>")
                    return

        print(BANNER)
        file_count = self.scan_context()
        print(f"ready. ({file_count} local files scanned)")
        print("Type 'menu' for options or just start talking.")
        
        while True:
            try:
                # Dynamic Prompt
                prompt_label = f"omni ({self.active_brain})" if self.active_brain else "omni"
                user_input = input(f"\n{prompt_label} > ").strip()
                
                if not user_input: continue
                
                # Check for "train" command in interactive mode too?
                if user_input.startswith("train "):
                    parts = user_input.split()
                    if len(parts) >= 3:
                        # Handle spaces in path?
                        # naive parse: train name path
                        name = parts[1]
                        path = parts[2]
                        self.train_brain(name, path)
                        continue
                
                action = self.route_intent(user_input)
                
                if action == "EXIT":
                    print("ðŸ‘‹ Shutting down.")
                    break
                elif action == "MENU":
                    self.show_menu()
                elif action == "STATUS":
                    print(f"ðŸ’¾ GPU Memory: [Safe]") # Mock
                    print(f"ðŸ“‚ Active Context: {len(self.context_files)} files")
                elif action.startswith("LOAD:"):
                    target = action.split(":")[1]
                    self.load_brain(target)
                elif action == "CHAT":
                    self.generate_response(user_input)
                    
            except KeyboardInterrupt:
                print("\nUse 'exit' to quit.")
            except Exception as e:
                print(f"Error: {e}")

if __name__ == "__main__":
    agent = OmniAgent()
    if len(sys.argv) > 1:
        if sys.argv[1] == "train":
            agent.run() # run() handles sys.argv logic now
        elif sys.argv[1] == "run":
            agent.run()
        else:
             print("Usage: omni [run|train ...]")
    else:
        agent.run()
